{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Te damos la bienvenida a Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laucio/colab/blob/main/Te_damos_la_bienvenida_a_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "# 1 Introducción\n",
        "\n",
        "Una gran Rama de HPC es el procesamiento de lenguaje natural, que es una rama de la inteligencia artificial que ayuda a las computadoras a entender, interpretar y manipular el lenguaje humano.\n",
        "\n",
        "En el siguiente ejercicio vamos a partir de una cadena de texto, a la que vamos a aplicar una cadena de funciones para normalizarlo.\n",
        "\n",
        "El procesamiento de lenguaje natural nos puede servir para entender el sentimiento del texto, poder realizar acciones computacionales en base a ello o darle una connotación tanto positiva o negativa a la oración analizada, entre otras aplicaciones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvgFR2jU6Dy"
      },
      "source": [
        "# Armado Ambiente\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WmA13U9VCpi",
        "outputId": "73c2bb8d-c3a3-44da-a1c1-8b07adff3a0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pycuda\n",
        "!pip install lorem\n",
        "!pip install contractions\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.7/dist-packages (2021.1)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.7/dist-packages (from pycuda) (2021.2.9)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.1.6)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (1.19.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Requirement already satisfied: lorem in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 42.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85452 sha256=7a2f56dd5d7c3f8e3b5c5eacc0dea5a4b7cbcd88403d49a3e7f9ccb26962d173\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya63N2YiVTDK",
        "outputId": "bb596565-755a-48ae-f71a-21e4cef06993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import lorem\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import contractions\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texto = \"\"\"Nosotros y nuestros socios almacenamos o accedemos a información en un dispositivo, tales como cookies, y procesamos datos personales, tales como identificadores únicos e información estándar enviada por un dispositivo, para anuncios y contenido personalizados, medición de anuncios y del contenido e información sobre el público, así como para desarrollar y mejorar productos.\n",
        "Con su permiso, nosotros y nuestros socios podemos utilizar datos de localización geográfica precisa e identificación mediante las características de dispositivos. Puede hacer clic para otorgarnos su consentimiento a nosotros y a nuestros socios para que llevemos a cabo el procesamiento previamente descrito. De forma alternativa, puede acceder a información más detallada y cambiar sus preferencias antes de otorgar o negar su consentimiento.\n",
        "Tenga en cuenta que algún procesamiento de sus datos personales puede no requerir de su consentimiento, pero usted tiene el derecho de rechazar tal procesamiento. Sus preferencias se aplicarán solo a este sitio web. Puede cambiar sus preferencias en cualquier momento entrando de nuevo en este sitio web o visitando nuestra política de privacidad.\"\"\"\n",
        "\n",
        "texto_tokenizado = nltk.word_tokenize(texto)\n",
        "\n",
        "def borrar_caracteres_no_ascii(palabras):\n",
        "    \"Borro caracteres no ascii\"\n",
        "    nuevas_palabras = []\n",
        "    for palabra in palabras:\n",
        "        nueva_palabra = unicodedata.normalize('NFKD', palabra).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        nuevas_palabras.append(nueva_palabra)\n",
        "    return nuevas_palabras\n",
        "\n",
        "\n",
        "def to_lowercase(palabras):\n",
        "    \"Convierto todas las palabras a lowercase\"\n",
        "    nuevas_palabras = []\n",
        "    for palabra in palabras:\n",
        "        nueva_palabra = palabra.lower()\n",
        "        nuevas_palabras.append(nueva_palabra)\n",
        "    return nuevas_palabras\n",
        "\n",
        "def remover_puntuacion(palabras):\n",
        "    \"Borro signos de puntuacion\"\n",
        "    nuevas_palabras = []\n",
        "    for palabra in palabras:\n",
        "        nueva_palabra = re.sub(r'[^\\w\\s]', '', palabra)\n",
        "        if nueva_palabra != '':\n",
        "            nuevas_palabras.append(nueva_palabra)\n",
        "    return nuevas_palabras\n",
        "\n",
        "def reemplazo_numeros(palabras):\n",
        "    \"Reemplazo los numeros por su denominacion en texto\"\n",
        "    p = inflect.engine()\n",
        "    nuevas_palabras = []\n",
        "    for palabra in palabras:\n",
        "        if palabra.isdigit():\n",
        "            nueva_palabra = p.number_to_palabras(palabra)\n",
        "            nuevas_palabras.append(nueva_palabra)\n",
        "        else:\n",
        "            nuevas_palabras.append(palabra)\n",
        "    return nuevas_palabras\n",
        "\n",
        "def borrar_stopwords(palabras):\n",
        "    \"\"\"StopWords son palabras que no aplican mucho significado al texto y pueden ser borradas como La El Etc\"\"\"\n",
        "    nuevas_palabras = []\n",
        "    for palabra in palabras:\n",
        "        if palabra not in stopwords.words('spanish'):\n",
        "            nuevas_palabras.append(palabra)\n",
        "    return nuevas_palabras\n",
        "\n",
        "\n",
        "def normalizar(palabras):\n",
        "    palabras = borrar_caracteres_no_ascii(palabras)\n",
        "    palabras = to_lowercase(palabras)\n",
        "    palabras = remover_puntuacion(palabras)\n",
        "    palabras = reemplazo_numeros(palabras)\n",
        "    palabras = borrar_stopwords(palabras)\n",
        "    return palabras\n",
        "\n",
        "palabras_normalizadas = normalizar(texto_tokenizado)\n",
        "\n",
        "print(palabras_normalizadas)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['socios', 'almacenamos', 'accedemos', 'informacion', 'dispositivo', 'tales', 'cookies', 'procesamos', 'datos', 'personales', 'tales', 'identificadores', 'unicos', 'informacion', 'estandar', 'enviada', 'dispositivo', 'anuncios', 'contenido', 'personalizados', 'medicion', 'anuncios', 'contenido', 'informacion', 'publico', 'asi', 'desarrollar', 'mejorar', 'productos', 'permiso', 'socios', 'podemos', 'utilizar', 'datos', 'localizacion', 'geografica', 'precisa', 'identificacion', 'mediante', 'caracteristicas', 'dispositivos', 'puede', 'hacer', 'clic', 'otorgarnos', 'consentimiento', 'socios', 'llevemos', 'cabo', 'procesamiento', 'previamente', 'descrito', 'forma', 'alternativa', 'puede', 'acceder', 'informacion', 'mas', 'detallada', 'cambiar', 'preferencias', 'otorgar', 'negar', 'consentimiento', 'cuenta', 'algun', 'procesamiento', 'datos', 'personales', 'puede', 'requerir', 'consentimiento', 'usted', 'derecho', 'rechazar', 'tal', 'procesamiento', 'preferencias', 'aplicaran', 'solo', 'sitio', 'web', 'puede', 'cambiar', 'preferencias', 'cualquier', 'momento', 'entrando', 'nuevo', 'sitio', 'web', 'visitando', 'politica', 'privacidad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egYbTVswTOod"
      },
      "source": [
        "#Conclusiones\n",
        "\n",
        "Con el ejercicio planteado y resuelto, podemos ver lo poderoso que es el lenguaje python y sus infinitas bibliotecas.\n",
        "\n",
        "El hecho de poder importar funcionalidad complejas y ejecutarlas facilmente, nos permite abstraernos de la complejidad del algoritmo y enfocarnos en el problema general.\n",
        "\n",
        "Una vez hecha la normalizacion del texto, existen muchas mas bibliotecas python para seguir procesando y darle valor y sentido a la normalizacion del texto.\n",
        "\n",
        "Algunas de estas bibliotecas son: TextBlob, Standford CoreNLP, Spacy, Textacy, Gensim, pyLDAvis etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGYucG9tU0IW"
      },
      "source": [
        "#Bibliografía\n",
        "\n",
        "[Que es el Procesamiento Natural? - IDB blog](https://blogs.iadb.org/conocimiento-abierto/es/que-es-el-procesamiento-de-lenguaje-natural-y-como-ponerlo-en-practica-con-recursos-abiertos/)\n",
        "\n",
        "[Ntlk docs](https://www.nltk.org/)\n",
        "\n",
        "[Procesamiento lenguaje natural - Universidad Catalunya](http://datascience.recursos.uoc.edu/es/procesamiento-del-lenguaje-natural-nlp/)"
      ]
    }
  ]
}